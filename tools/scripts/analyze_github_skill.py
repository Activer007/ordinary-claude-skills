#!/usr/bin/env python3
"""
åˆ†æ GitHub ä¸Šçš„æŠ€èƒ½

åŠŸèƒ½ï¼š
1. ä» GitHub URL ç›´æ¥åˆ†ææŠ€èƒ½è´¨é‡
2. æ”¯æŒæ‰¹é‡åˆ†æï¼ˆä»æ–‡ä»¶è¯»å– URLsï¼‰
3. è‡ªåŠ¨ç¼“å­˜ä¸‹è½½çš„æŠ€èƒ½
4. ç”Ÿæˆè¯¦ç»†çš„è¯„åˆ†æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    # åˆ†æå•ä¸ªæŠ€èƒ½
    python scripts/analyze_github_skill.py https://github.com/.../skill-name

    # æ‰¹é‡åˆ†æï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰
    python scripts/analyze_github_skill.py --batch urls.txt

    # æ¸…ç†ç¼“å­˜
    python scripts/analyze_github_skill.py --clear-cache

    # æ˜¾ç¤ºå¸®åŠ©
    python scripts/analyze_github_skill.py --help
"""

import sys
import argparse
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from analyzer.skill_analyzer import SkillAnalyzer
from analyzer.github_fetcher import GitHubSkillFetcher


def print_separator(char='=', length=60):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def analyze_single_url(url: str, output_file: str = None):
    """
    åˆ†æå•ä¸ª GitHub URL

    Args:
        url: GitHub æŠ€èƒ½ URL
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    print_separator()
    print(f"ğŸ” åˆ†æ GitHub æŠ€èƒ½")
    print_separator()
    print(f"\nURL: {url}")
    print(f"\nå¼€å§‹åˆ†æ...\n")

    try:
        # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
        analyzer = SkillAnalyzer.from_github_url(url)
        result = analyzer.analyze()

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if 'error' in result:
            print(f"\nâŒ åˆ†æå¤±è´¥: {result['error']}\n")
            return None

        # è¾“å‡ºç»“æœ
        print_separator()
        print("ğŸ“Š åˆ†æç»“æœ")
        print_separator()

        print(f"\næŠ€èƒ½åç§°: {result['skill_name']}")
        print(f"æœ¬åœ°è·¯å¾„: {result['skill_path']}")

        # æ€»åˆ†å’Œç­‰çº§
        print(f"\n{'='*40}")
        print(f"æ€»åˆ†: {result['total_score']}/100")
        print(f"ç­‰çº§: {result['grade']}")
        print(f"{'='*40}\n")

        # è¯¦ç»†è¯„åˆ†
        scores = result['scores']
        print("è¯¦ç»†è¯„åˆ†:")
        print(f"  â”œâ”€ å†…å®¹è´¨é‡: {scores['content']['total']:2}/50")
        print(f"  â”œâ”€ æŠ€æœ¯å®ç°: {scores['technical']['total']:2}/30")
        print(f"  â”œâ”€ ç»´æŠ¤æ€§:   {scores['maintenance']['total']:2}/10")
        print(f"  â””â”€ ç”¨æˆ·ä½“éªŒ: {scores['ux']['total']:2}/10")

        # æ”¹è¿›å»ºè®®
        recommendations = result.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(recommendations)} é¡¹):")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print(f"\nâœ… å¤ªæ£’äº†ï¼æ²¡æœ‰æ”¹è¿›å»ºè®®ï¼ŒæŠ€èƒ½è´¨é‡å¾ˆé«˜ã€‚")

        # ä¿å­˜ JSON ç»“æœ
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # æ·»åŠ æ—¶é—´æˆ³
            result['analyzed_at'] = datetime.now().isoformat()
            result['github_url'] = url

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        print()
        print_separator()

        return result

    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return None


def analyze_batch(file_path: Path, output_dir: str = None):
    """
    æ‰¹é‡åˆ†æ URLsï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰

    Args:
        file_path: åŒ…å« URLs çš„æ–‡æœ¬æ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    """
    print_separator()
    print(f"ğŸ“‹ æ‰¹é‡åˆ†ææ¨¡å¼")
    print_separator()

    # è¯»å– URLs
    if not file_path.exists():
        print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n")
        return

    urls = file_path.read_text(encoding='utf-8').strip().split('\n')
    urls = [url.strip() for url in urls if url.strip() and not url.startswith('#')]

    if not urls:
        print(f"\nâŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ URL\n")
        return

    print(f"\nä»æ–‡ä»¶è¯»å–: {file_path}")
    print(f"å…± {len(urls)} ä¸ª URL\n")

    results = []
    failures = []

    # é€ä¸ªåˆ†æ
    for i, url in enumerate(urls, 1):
        print_separator()
        print(f"[{i}/{len(urls)}] {url}")
        print_separator()

        result = analyze_single_url(url)

        if result and 'error' not in result:
            results.append(result)
            print(f"âœ… æˆåŠŸ: {result['skill_name']} ({result['total_score']}/100)")
        else:
            failures.append({'url': url, 'error': result.get('error', 'Unknown error') if result else 'Download failed'})
            print(f"âŒ å¤±è´¥")

        print()

    # æ±‡æ€»æŠ¥å‘Š
    print_separator()
    print("ğŸ“ˆ æ‰¹é‡åˆ†ææ±‡æ€»")
    print_separator()

    print(f"\næ€»è®¡: {len(urls)} ä¸ª URL")
    print(f"æˆåŠŸ: {len(results)} ä¸ª")
    print(f"å¤±è´¥: {len(failures)} ä¸ª")

    if results:
        scores = [r['total_score'] for r in results]
        grades = [r['grade'] for r in results]

        print(f"\nè¯„åˆ†ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†: {sum(scores)/len(scores):.1f}/100")
        print(f"  æœ€é«˜åˆ†: {max(scores)}/100")
        print(f"  æœ€ä½åˆ†: {min(scores)}/100")

        print(f"\nç­‰çº§åˆ†å¸ƒ:")
        for grade in ['S', 'A', 'B', 'C', 'D']:
            count = grades.count(grade)
            if count > 0:
                percentage = count / len(results) * 100
                print(f"  {grade} çº§: {count} ä¸ª ({percentage:.1f}%)")

        # Top 3
        sorted_results = sorted(results, key=lambda x: x['total_score'], reverse=True)
        print(f"\nğŸ† Top 3 é«˜åˆ†æŠ€èƒ½:")
        for i, r in enumerate(sorted_results[:3], 1):
            print(f"  {i}. {r['skill_name']} ({r['total_score']}/100 - {r['grade']})")

        # ä¿å­˜æ‰¹é‡ç»“æœ
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            batch_file = output_path / f"batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(batch_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'analyzed_at': datetime.now().isoformat(),
                    'total_count': len(urls),
                    'success_count': len(results),
                    'failure_count': len(failures),
                    'results': results,
                    'failures': failures
                }, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ“ æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°: {batch_file}")

    if failures:
        print(f"\nâš ï¸  å¤±è´¥çš„ URL:")
        for f in failures:
            print(f"  - {f['url']}")
            print(f"    åŸå› : {f['error']}")

    print()
    print_separator()


def clear_cache(skill_name: str = None):
    """
    æ¸…ç†ç¼“å­˜

    Args:
        skill_name: æŒ‡å®šæŠ€èƒ½åç§°ï¼ŒNone è¡¨ç¤ºæ¸…ç†å…¨éƒ¨
    """
    print_separator()
    print("ğŸ—‘ï¸  æ¸…ç†ç¼“å­˜")
    print_separator()

    fetcher = GitHubSkillFetcher()

    if skill_name:
        print(f"\næ¸…ç†æŠ€èƒ½: {skill_name}")
    else:
        print(f"\næ¸…ç†å…¨éƒ¨ç¼“å­˜")

    fetcher.clear_cache(skill_name)
    print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='åˆ†æ GitHub ä¸Šçš„æŠ€èƒ½è´¨é‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†æå•ä¸ªæŠ€èƒ½
  %(prog)s https://github.com/anthropics/claude-cookbooks/tree/main/skills/custom_skills/applying-brand-guidelines

  # æ‰¹é‡åˆ†æ
  %(prog)s --batch urls.txt

  # ä¿å­˜ç»“æœåˆ° JSON
  %(prog)s <URL> --output result.json

  # æ‰¹é‡åˆ†æå¹¶ä¿å­˜
  %(prog)s --batch urls.txt --output-dir reports/

  # æ¸…ç†å…¨éƒ¨ç¼“å­˜
  %(prog)s --clear-cache

  # æ¸…ç†ç‰¹å®šæŠ€èƒ½ç¼“å­˜
  %(prog)s --clear-cache --skill-name applying-brand-guidelines
        """
    )

    parser.add_argument('url', nargs='?', help='GitHub æŠ€èƒ½ URL')
    parser.add_argument('--batch', type=Path, help='æ‰¹é‡åˆ†æï¼ˆä»æ–‡ä»¶è¯»å– URLsï¼‰')
    parser.add_argument('--output', help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªåˆ†æï¼‰')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•ï¼ˆæ‰¹é‡åˆ†æï¼‰')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç†ç¼“å­˜')
    parser.add_argument('--skill-name', help='æŒ‡å®šæŠ€èƒ½åç§°ï¼ˆç”¨äºæ¸…ç†ç¼“å­˜ï¼‰')

    args = parser.parse_args()

    # æ¸…ç†ç¼“å­˜
    if args.clear_cache:
        clear_cache(args.skill_name)
        return

    # æ‰¹é‡åˆ†æ
    if args.batch:
        analyze_batch(args.batch, args.output_dir)
        return

    # å•ä¸ªåˆ†æ
    if args.url:
        analyze_single_url(args.url, args.output)
        return

    # æ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©
    parser.print_help()


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­\n")
        sys.exit(1)
