# 评分系统升级路线图：从“规则校验”到“智能语义评审”

## 1. 愿景与目标 (Vision)
将现有的基于 Python 规则的静态评分引擎升级为 **“混合动力·多 Agent 专家评审架构 (Hybrid Multi-Agent Evaluator)”**。
*   **核心转变**：从“统计计数器”进化为“语义评审官”。
*   **最终价值**：不仅给出分数，更提供具有深度逻辑的改进建议，并为未来的 **“Skills Master（自动进化引擎）”** 提供评估基准。

---

## 2. 核心架构：三层评估流水线

### 第一层：自动化事实传感器 (Rule-based Sensors) —— 效率保障
*   **定位**：利用 Python 脚本进行“硬指标”扫描。
*   **机制**：**可插拔传感器 (Plugable Sensors)**。通过配置文件灵活增减检测项。
*   **输出**：**JSON 技术事实清单 (Technical Fact Sheet)**。
    *   *例如*：文件结构、代码圈复杂度、提示词字数、异常处理覆盖率、特定库引用情况。
*   **作用**：作为 AI 审计的“眼睛”，屏蔽重复性的数数工作，降低 Token 成本。

### 第二层：多 Agent 专家审计 (Multi-Agent Audit) —— 深度理解
*   **定位**：针对第一层提取的事实，由多个专项 AI 角色进行深层研判。
*   **角色设定**：
    *   **架构师 Agent**：审计代码逻辑健壮性、安全性及 Prompt 注入风险。
    *   **产品/UX Agent**：审计 README 的易用性及 Skill 的交互逻辑质量。
    *   **稀缺性 Agent**：对比库中现有 Skill，判断原创性与独特性。
*   **机制**：采用 **Few-shot Prompting**，为每个 Agent 提供“好坏对照组”。

### 第三层：主审官汇总与归因 (Reasoning & Attribution) —— 公信力保障
*   **定位**：汇总专家意见，计算最终得分，并生成归因报告。
*   **核心产出**：**带有“改进蓝图”的评审报告**。解释每一个扣分项背后的逻辑，而不仅仅是一个数字。

---

## 3. 关键执行策略

### 策略 A：稳定性保障（针对 AI 的随机性）
为了确保评分的稳定与一致，系统需遵循：
1.  **锚点比对法 (Anchor-based Scoring)**：在 Prompt 中内置“黄金样本（Golden Samples）”，让 AI 将待测 Skill 与 100分/60分/30分 的标杆进行横向对比。
2.  **推理链强制化 (Reasoning Chain)**：要求 AI 必须先列举事实依据，再给出分值。分值必须与推理链条逻辑自洽。
3.  **结构化输出控制**：利用 JSON Schema 强制 AI 输出固定维度的分值，杜绝模糊评价。

### 策略 B：灵活性与成本优化（针对大规模 Skill）
1.  **阶梯式过滤机制**：
    *   **Level 1**：所有 Skill 跑脚本初筛。
    *   **Level 2**：通过初筛（如 > 60分）或有更新的 Skill 进入 AI 专家审计。
2.  **传感器解耦**：指标的增加只需编写独立的子脚本并注册到 `Sensors` 列表，无需修改主逻辑。

---

## 4. 未来愿景：Skills Master 进化闭环

该评分系统将作为 **Skills Master（自动创作大师）** 的“适应度函数”：
1.  **评估 (Evaluate)**：系统指出 Skill 的薄弱点（如：缺乏边界处理）。
2.  **创作 (Refactor)**：Skills Master 根据评审报告中的改进蓝图，自动重写代码或优化提示词。
3.  **验证 (Verify)**：再次评分，直到 Skill 达到“金牌”标准。
4.  **杂交与创新**：通过分析高分 Skill 的技术事实，自动组合不同 Skill 的优势，创造全新的复合技能。

---

## 5. 近期行动方向 (Next Steps)
1.  **传感器原子化**：整理现有的 `tools/analyzer/` 逻辑，将其改造为独立的“事实提取器”。
2.  **定义“技术事实规约”**：确定哪些关键信息是必须喂给 AI 的（例如：提取 Prompt 的核心逻辑块）。
3.  **专家 Prompt 库构建**：为“架构师”、“UX专家”等角色编写第一版评审准则。
